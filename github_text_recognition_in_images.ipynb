{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "github text recognition in images",
      "provenance": [],
      "authorship_tag": "ABX9TyP1r/K36jTygWV6spCYJnAS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgite03/image-preprocessing/blob/main/github_text_recognition_in_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llVBHQv8jTxP"
      },
      "source": [
        "# Basic cv2 functionality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFjWtgk1u2gp"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# or whatever filepath the image has, this was done on google colab\n",
        "filepath = '/content/img_1.jpg'\n",
        "\n",
        "#NOT GRAYSCALE\n",
        "#img = cv2.imread(filepath)\n",
        "#GRAYSCALE\n",
        "img = cv2.imread(filepath, 0)\n",
        "\n",
        "cv2_imshow(img)\n",
        "\n",
        "#ALTERNATE WAY OF SHOWING IMAGES, INSTEAD OF AS AN IMAGE ITS PYPLOT - doesn't really work w cv grayscale\n",
        "#plt.imshow(img)\n",
        "\n",
        "#imwrite writes to a file\n",
        "cv2.imwrite('MyPicGray.jpg', img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGww1_oulAbJ"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "import pytesseract\n",
        "\n",
        "import difflib\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DMqE0iuhCSC"
      },
      "source": [
        "#takes in and image file and its corresponding text file, and outputs the ratio of how similar they are\n",
        "def read_image_and_compare_truth(img, textfile,flag):\n",
        "  text = \"\"\n",
        "\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # do something called thresholding - under the first number everything turns a specific color - cv2.THRESH_BINARY_INV and cv2.THRESH_OTSU were the two flags tested\n",
        "  ret, thresh1 = cv2.threshold(gray, 120, 255, cv2.THRESH_OTSU)\n",
        "  \n",
        "  #plt.imshow(thresh1)\n",
        "\n",
        "  # Specify structure shape and kernel size. \n",
        "  # Kernel size increases or decreases the area \n",
        "  # of the rectangle to be detected.\n",
        "  # A smaller value like (10, 10) will detect \n",
        "  # each word instead of a sentence\n",
        "  rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20, 20))\n",
        "  kernel = np.ones((20,20),np.float32)/400\n",
        "\n",
        "\n",
        "    # dst = cv2.filter2D(img,-1,kernel)\n",
        "    # blur = cv2.blur(img,(20,20))\n",
        "\n",
        "\n",
        "\n",
        "  #dilate the image\n",
        "  if flag == 1:\n",
        "    dilation = cv2.dilate(thresh1, rect_kernel, iterations = 1)\n",
        "  else:\n",
        "    dilation = cv2.dilate(thresh1, rect_kernel, iterations = 3)\n",
        "    \n",
        "  #finding contours  \n",
        "  contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL, \n",
        "                                                  cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "  # Creating a copy of image\n",
        "  im2 = img.copy()\n",
        "    \n",
        "\n",
        "    \n",
        "  # Looping through the identified contours\n",
        "  # Then rectangular part is cropped and passed on\n",
        "  # to pytesseract for extracting text from it\n",
        "  # Extracted text is then written into the text file\n",
        "  for cnt in contours:\n",
        "      x, y, w, h = cv2.boundingRect(cnt)\n",
        "        \n",
        "      # Drawing a rectangle on copied image\n",
        "      rect = cv2.rectangle(im2, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
        "        \n",
        "      # Cropping the text block for giving input to OCR\n",
        "      cropped = im2[y:y + h, x:x + w]\n",
        "        \n",
        "      # Open the file in append mode\n",
        "      #file = open(\"recognized.txt\", \"a\")\n",
        "        \n",
        "      # Apply OCR on the cropped image\n",
        "      text += pytesseract.image_to_string(cropped)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  gt = textfile\n",
        "  filelines = gt.readlines()\n",
        "  ret_str = \"\"\n",
        "  for line in filelines:                \n",
        "      splitline = line.split()\n",
        "      words = splitline[4]\n",
        "\n",
        "      ret_str += words\n",
        "\n",
        "\n",
        "  ratio = difflib.SequenceMatcher(None, ret_str,text).ratio()\n",
        "  return ratio\n",
        "\n",
        "img = cv2.imread(\"/content/100.jpg\")\n",
        "textfile = open(\"/content/gt_100.txt\", mode=\"r\")\n",
        "\n",
        "#flag was changed depending on whether thesholding, dilation, or \n",
        "flag = 0\n",
        "read_image_and_compare_truth(img, textfile,flag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB98_-EVhC11"
      },
      "source": [
        "#testing differences in dilation, can be changed for blur and thresholding\n",
        "similarity = []\n",
        "imgs = []\n",
        "ones = []\n",
        "threes = []\n",
        "for i in range (100,131):\n",
        "  imgs += [i]\n",
        "\n",
        "  tex = open(f\"/content/gt_{i}.txt\", mode = 'r')\n",
        "  imag = cv2.imread(f\"/content/{i}.jpg\")\n",
        "  one = read_image_and_compare_truth(imag, tex, 1)\n",
        "  ones += [one]\n",
        "\n",
        "  tex = open(f\"/content/gt_{i}.txt\", mode = 'r')\n",
        "  imag = cv2.imread(f\"/content/{i}.jpg\")\n",
        "  three = read_image_and_compare_truth(imag, tex, 3)\n",
        "  threes += [threes]\n",
        "\n",
        "  similarity += [three-one]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqez_esWhE2t"
      },
      "source": [
        "plt.plot(imgs, similarity, \"co\")\n",
        "\n",
        "\n",
        "# plt.hist(similarity, 10)\n",
        "# plt.xlim(0,1)\n",
        "plt.axhline(y=0, color=\"black\", linestyle=\"-\")\n",
        "plt.title('Dilation Differences(3 Iterations - 1 Iteration)')\n",
        "plt.xlabel('Picture number')\n",
        "plt.ylabel('Fraction of Matching Characters')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJxTBWRbhLGk"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(otsu, 10, label=\"OTSU\")\n",
        "\n",
        "plt.xlabel('Pic Number')\n",
        "plt.ylabel('Fraction of Matching Characters')\n",
        "plt.title('Thresholding Differences')\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.hist(binary, 10, label=\"BINARY_INV\")\n",
        "plt.xlabel('Pic Number')\n",
        "plt.ylabel('Fraction of Matching Characters')\n",
        "plt.title('Thresholding Differences')\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIjMaDNF1m6o"
      },
      "source": [
        "# easy ocr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOvEMTSN1uYF"
      },
      "source": [
        "!pip install easyocr\n",
        "\n",
        "import easyocr\n",
        "reader = easyocr.Reader(['en']) # need to run only once to load model into memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNk9OxuD1wjx"
      },
      "source": [
        "result = reader.readtext(\"/content/167.jpg\", detail=0, paragraph= True)\n",
        "print(result)\n",
        "\n",
        "img = cv2.imread(\"/content/167.jpg\")\n",
        "cv2_imshow(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}